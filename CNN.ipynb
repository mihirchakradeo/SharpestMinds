{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a ConvNet in PyTorch for Human Action Recognition on UCI dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,sampler,Dataset\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import timeit\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "In this part, we will load the action recognition dataset for the neural network. In order to load data from our custom dataset, we need to write a custom Dataloader. If you put q3_2_data.mat, /valClips,/trainClips,/testClips under the folder of ./data/ , you do not need to change anything in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the labels of the dataset, you should write your path of the q3_2_data.mat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7770\n",
      "2230\n"
     ]
    }
   ],
   "source": [
    "label_mat=scipy.io.loadmat('./data/q3_2_data.mat')\n",
    "label_train=label_mat['trLb']\n",
    "print(len(label_train))\n",
    "label_val=label_mat['valLb']\n",
    "print(len(label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "torch.utils.data.Dataset is an abstract class representing a dataset. The custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "    __len__ so that len(dataset) returns the size of the dataset.\n",
    "    __getitem__ to support the indexing such that dataset[i] can be used to get ith sample\n",
    "\n",
    "Letâ€™s create a dataset class for our action recognition dataset. We will read images in __getitem__. This is memory efficient because all the images are not stored in the memory at once but read as required.\n",
    "\n",
    "Sample of our dataset will be a dict {'image':image,'img_path':img_path,'Label':Label}. Our datset will take an optional argument transform so that any required processing can be applied on the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActionDataset(Dataset):\n",
    "    \"\"\"Action dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,  root_dir,labels=[], transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            labels(list): labels if images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.length=len(os.listdir(self.root_dir))\n",
    "        self.labels=labels\n",
    "    def __len__(self):\n",
    "        return self.length*3\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        folder=idx/3+1\n",
    "        imidx=idx%3+1\n",
    "        folder = int(folder)\n",
    "        folder=format(folder,'05d')\n",
    "        \n",
    "        imgname=str(imidx)+'.jpg'\n",
    "        img_path = os.path.join(self.root_dir,\n",
    "                                folder,imgname)\n",
    "        image = Image.open(img_path)\n",
    "        if len(self.labels)!=0:\n",
    "            Label=self.labels[int(idx/3)][0]-1\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if len(self.labels)!=0:\n",
    "            sample={'image':image,'img_path':img_path,'Label':Label}\n",
    "        else:\n",
    "            sample={'image':image,'img_path':img_path}\n",
    "        return sample\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over the dataset by a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00001/1.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00001/2.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00001/3.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00002/1.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00002/2.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00002/3.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00003/1.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00003/2.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00003/3.jpg\n",
      "torch.Size([3, 64, 64])\n",
      "0.0\n",
      "./data/trainClips/00004/1.jpg\n"
     ]
    }
   ],
   "source": [
    "image_dataset=ActionDataset(root_dir='./data/trainClips/',\\\n",
    "                            labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "#iterating though the dataset\n",
    "for i in range(10):\n",
    "    sample=image_dataset[i]\n",
    "    print(sample['image'].shape)\n",
    "    print(sample['Label'])\n",
    "    print(sample['img_path'])\n",
    "     \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05571/1.jpg', './data/trainClips/03060/3.jpg', './data/trainClips/06254/2.jpg', './data/trainClips/00159/1.jpg'] \n",
      " 6\n",
      " 3\n",
      " 7\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "1 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04265/2.jpg', './data/trainClips/05144/2.jpg', './data/trainClips/02134/3.jpg', './data/trainClips/03774/2.jpg'] \n",
      " 5\n",
      " 6\n",
      " 2\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "2 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03296/3.jpg', './data/trainClips/05521/2.jpg', './data/trainClips/00811/3.jpg', './data/trainClips/02166/2.jpg'] \n",
      " 3\n",
      " 6\n",
      " 0\n",
      " 2\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "3 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03027/1.jpg', './data/trainClips/06928/3.jpg', './data/trainClips/00378/3.jpg', './data/trainClips/05205/3.jpg'] \n",
      " 3\n",
      " 8\n",
      " 0\n",
      " 6\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "4 torch.Size([4, 3, 64, 64]) ['./data/trainClips/06140/2.jpg', './data/trainClips/05659/1.jpg', './data/trainClips/00545/2.jpg', './data/trainClips/03807/3.jpg'] \n",
      " 7\n",
      " 7\n",
      " 0\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "5 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04687/2.jpg', './data/trainClips/03561/1.jpg', './data/trainClips/02758/3.jpg', './data/trainClips/05310/3.jpg'] \n",
      " 5\n",
      " 4\n",
      " 3\n",
      " 6\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "6 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05596/3.jpg', './data/trainClips/00990/3.jpg', './data/trainClips/06335/3.jpg', './data/trainClips/00619/2.jpg'] \n",
      " 6\n",
      " 1\n",
      " 7\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "7 torch.Size([4, 3, 64, 64]) ['./data/trainClips/01979/2.jpg', './data/trainClips/04761/3.jpg', './data/trainClips/01347/3.jpg', './data/trainClips/01213/3.jpg'] \n",
      " 2\n",
      " 5\n",
      " 1\n",
      " 1\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "8 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05501/3.jpg', './data/trainClips/05835/2.jpg', './data/trainClips/06271/3.jpg', './data/trainClips/04989/1.jpg'] \n",
      " 6\n",
      " 7\n",
      " 7\n",
      " 6\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "9 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04692/2.jpg', './data/trainClips/06226/1.jpg', './data/trainClips/03345/3.jpg', './data/trainClips/00802/2.jpg'] \n",
      " 5\n",
      " 7\n",
      " 3\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "10 torch.Size([4, 3, 64, 64]) ['./data/trainClips/00132/3.jpg', './data/trainClips/06240/2.jpg', './data/trainClips/06974/1.jpg', './data/trainClips/03946/3.jpg'] \n",
      " 0\n",
      " 7\n",
      " 8\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "11 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05030/3.jpg', './data/trainClips/06855/2.jpg', './data/trainClips/01012/2.jpg', './data/trainClips/07512/2.jpg'] \n",
      " 6\n",
      " 8\n",
      " 1\n",
      " 9\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "12 torch.Size([4, 3, 64, 64]) ['./data/trainClips/01708/3.jpg', './data/trainClips/03422/2.jpg', './data/trainClips/00901/1.jpg', './data/trainClips/07191/1.jpg'] \n",
      " 1\n",
      " 3\n",
      " 0\n",
      " 9\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "13 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04112/1.jpg', './data/trainClips/04877/3.jpg', './data/trainClips/00238/3.jpg', './data/trainClips/00951/2.jpg'] \n",
      " 4\n",
      " 5\n",
      " 0\n",
      " 1\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "14 torch.Size([4, 3, 64, 64]) ['./data/trainClips/07473/2.jpg', './data/trainClips/01190/1.jpg', './data/trainClips/00877/3.jpg', './data/trainClips/03615/1.jpg'] \n",
      " 9\n",
      " 1\n",
      " 0\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "15 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04377/2.jpg', './data/trainClips/00868/2.jpg', './data/trainClips/01401/3.jpg', './data/trainClips/03627/2.jpg'] \n",
      " 5\n",
      " 0\n",
      " 1\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "16 torch.Size([4, 3, 64, 64]) ['./data/trainClips/02427/2.jpg', './data/trainClips/04734/1.jpg', './data/trainClips/06917/1.jpg', './data/trainClips/02733/3.jpg'] \n",
      " 2\n",
      " 5\n",
      " 8\n",
      " 3\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "17 torch.Size([4, 3, 64, 64]) ['./data/trainClips/03096/3.jpg', './data/trainClips/02882/3.jpg', './data/trainClips/02208/2.jpg', './data/trainClips/00612/3.jpg'] \n",
      " 3\n",
      " 3\n",
      " 2\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "18 torch.Size([4, 3, 64, 64]) ['./data/trainClips/04242/3.jpg', './data/trainClips/06156/2.jpg', './data/trainClips/06037/3.jpg', './data/trainClips/00472/2.jpg'] \n",
      " 5\n",
      " 7\n",
      " 7\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "19 torch.Size([4, 3, 64, 64]) ['./data/trainClips/05110/1.jpg', './data/trainClips/02863/3.jpg', './data/trainClips/07353/3.jpg', './data/trainClips/03347/2.jpg'] \n",
      " 6\n",
      " 3\n",
      " 9\n",
      " 3\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "20 torch.Size([4, 3, 64, 64]) ['./data/trainClips/06112/2.jpg', './data/trainClips/04993/3.jpg', './data/trainClips/03734/1.jpg', './data/trainClips/06695/1.jpg'] \n",
      " 7\n",
      " 6\n",
      " 4\n",
      " 8\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "21 torch.Size([4, 3, 64, 64]) ['./data/trainClips/00975/2.jpg', './data/trainClips/01148/2.jpg', './data/trainClips/00884/3.jpg', './data/trainClips/07612/2.jpg'] \n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 9\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_dataloader = DataLoader(image_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(image_dataloader):\n",
    "    sample['image']=sample['image'].cuda()\n",
    "    print(i,sample['image'].shape,sample['img_path'],sample['Label'])\n",
    "    if i>20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders for the training, validationg and testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_train=ActionDataset(root_dir='./data/trainClips/',labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "image_dataloader_train = DataLoader(image_dataset_train, batch_size=32,\n",
    "                        shuffle=True, num_workers=4)\n",
    "image_dataset_val=ActionDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "image_dataloader_val = DataLoader(image_dataset_val, batch_size=32,\n",
    "                        shuffle=False, num_workers=4)\n",
    "image_dataset_test=ActionDataset(root_dir='./data/testClips/',labels=[],transform=T.ToTensor())\n",
    "\n",
    "image_dataloader_test = DataLoader(image_dataset_test, batch_size=32,\n",
    "                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor # the CPU datatype\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "# This is a little utility that we'll use to reset the model\n",
    "# if we want to re-initialize all our parameters\n",
    "def reset(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I input  data into fully connected affine layers, I want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, I use a \"Flatten\" operation to collapse the C x H x W values per representation into a single long vector. The Flatten function below first reads in the N, C, H, and W values from a given batch of data, and then returns a \"view\" of that data. \"View\" is analogous to numpy's \"reshape\" method: it reshapes x's dimensions to be N x ??, where ?? is allowed to be anything (in this case, it will be C x H x W, but we don't need to specify that explicitly). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, optimizer, dataloader, num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, sample in enumerate(dataloader):\n",
    "            x_var = Variable(sample['image']).cuda()\n",
    "            y_var = Variable(sample['Label'].long()).cuda()\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['image']).cuda()\n",
    "        y_var = sample['Label']\n",
    "        scores = model(x_var)\n",
    "        y_var=y_var.cpu()\n",
    "        _, preds = scores.data.max(1)#scores.data.cpu().max(1)\n",
    "        #print(preds)\n",
    "        #print(y_var)\n",
    "        preds = preds.cpu()\n",
    "        y_var=y_var.cpu()\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "#     y_var=y_var.cpu()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy of the model.\n",
    "\n",
    "Getting a training loss of around 1.0-1.2, and a validation accuracy of around 50-60%. If I train for more epochs, the performance improves past these numbers. However that is a sign of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 5\n",
      "t = 100, loss = 2.2323\n",
      "t = 200, loss = 2.1437\n",
      "t = 300, loss = 1.7899\n",
      "t = 400, loss = 1.7177\n",
      "t = 500, loss = 1.6700\n",
      "t = 600, loss = 1.3837\n",
      "t = 700, loss = 1.2119\n",
      "Starting epoch 2 / 5\n",
      "t = 100, loss = 1.3968\n",
      "t = 200, loss = 1.3100\n",
      "t = 300, loss = 1.2680\n",
      "t = 400, loss = 1.2891\n",
      "t = 500, loss = 0.8881\n",
      "t = 600, loss = 0.7787\n",
      "t = 700, loss = 1.3149\n",
      "Starting epoch 3 / 5\n",
      "t = 100, loss = 0.8357\n",
      "t = 200, loss = 0.7589\n",
      "t = 300, loss = 0.8152\n",
      "t = 400, loss = 0.6862\n",
      "t = 500, loss = 0.5893\n",
      "t = 600, loss = 0.5960\n",
      "t = 700, loss = 0.7713\n",
      "Starting epoch 4 / 5\n",
      "t = 100, loss = 0.8090\n",
      "t = 200, loss = 0.6732\n",
      "t = 300, loss = 0.5251\n",
      "t = 400, loss = 0.3244\n",
      "t = 500, loss = 0.6170\n",
      "t = 600, loss = 0.5740\n",
      "t = 700, loss = 0.8652\n",
      "Starting epoch 5 / 5\n",
      "t = 100, loss = 0.4717\n",
      "t = 200, loss = 0.4895\n",
      "t = 300, loss = 0.3519\n",
      "t = 400, loss = 0.4562\n",
      "t = 500, loss = 0.5509\n",
      "t = 600, loss = 0.3944\n",
      "t = 700, loss = 0.3992\n",
      "Got 19835 / 23310 correct (85.09)\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(12345)\n",
    "# fixed_model.cpu()\n",
    "fixed_model.apply(reset) \n",
    "fixed_model.train() \n",
    "train(fixed_model, loss_fn, optimizer,image_dataloader_train, num_epochs=5) \n",
    "check_accuracy(fixed_model, image_dataloader_train)# check accuracy on the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 3539 / 6690 correct (52.90)\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(fixed_model, image_dataloader_val)#check accuracy on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a better  model for action recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "t = 100, loss = 0.5821\n",
      "t = 200, loss = 0.6408\n",
      "t = 300, loss = 0.4928\n",
      "t = 400, loss = 0.5139\n",
      "t = 500, loss = 0.2888\n",
      "t = 600, loss = 0.7220\n",
      "t = 700, loss = 0.3304\n",
      "Starting epoch 2 / 10\n",
      "t = 100, loss = 0.4616\n",
      "t = 200, loss = 0.6672\n",
      "t = 300, loss = 0.4131\n",
      "t = 400, loss = 0.4303\n",
      "t = 500, loss = 0.4448\n",
      "t = 600, loss = 0.1903\n",
      "t = 700, loss = 0.2739\n",
      "Starting epoch 3 / 10\n",
      "t = 100, loss = 0.2287\n",
      "t = 200, loss = 0.3276\n",
      "t = 300, loss = 0.2743\n",
      "t = 400, loss = 0.5678\n",
      "t = 500, loss = 0.2609\n",
      "t = 600, loss = 0.2605\n",
      "t = 700, loss = 0.2144\n",
      "Starting epoch 4 / 10\n",
      "t = 100, loss = 0.2837\n",
      "t = 200, loss = 0.1910\n",
      "t = 300, loss = 0.2030\n",
      "t = 400, loss = 0.1438\n",
      "t = 500, loss = 0.2707\n",
      "t = 600, loss = 0.1373\n",
      "t = 700, loss = 0.2113\n",
      "Starting epoch 5 / 10\n",
      "t = 100, loss = 0.2774\n",
      "t = 200, loss = 0.1830\n",
      "t = 300, loss = 0.2819\n",
      "t = 400, loss = 0.1720\n",
      "t = 500, loss = 0.4199\n",
      "t = 600, loss = 0.0932\n",
      "t = 700, loss = 0.2310\n",
      "Starting epoch 6 / 10\n",
      "t = 100, loss = 0.3639\n",
      "t = 200, loss = 0.1214\n",
      "t = 300, loss = 0.2372\n",
      "t = 400, loss = 0.2410\n",
      "t = 500, loss = 0.2050\n",
      "t = 600, loss = 0.0688\n",
      "t = 700, loss = 0.2214\n",
      "Starting epoch 7 / 10\n",
      "t = 100, loss = 0.1598\n",
      "t = 200, loss = 0.2384\n",
      "t = 300, loss = 0.1217\n",
      "t = 400, loss = 0.3719\n",
      "t = 500, loss = 0.1817\n",
      "t = 600, loss = 0.1059\n",
      "t = 700, loss = 0.2027\n",
      "Starting epoch 8 / 10\n",
      "t = 100, loss = 0.2708\n",
      "t = 200, loss = 0.0717\n",
      "t = 300, loss = 0.0754\n",
      "t = 400, loss = 0.1933\n",
      "t = 500, loss = 0.2308\n",
      "t = 600, loss = 0.1099\n",
      "t = 700, loss = 0.1044\n",
      "Starting epoch 9 / 10\n",
      "t = 100, loss = 0.0599\n",
      "t = 200, loss = 0.0824\n",
      "t = 300, loss = 0.1318\n",
      "t = 400, loss = 0.1693\n",
      "t = 500, loss = 0.1586\n",
      "t = 600, loss = 0.3106\n",
      "t = 700, loss = 0.1577\n",
      "Starting epoch 10 / 10\n",
      "t = 100, loss = 0.1201\n",
      "t = 200, loss = 0.1284\n",
      "t = 300, loss = 0.0610\n",
      "t = 400, loss = 0.1702\n",
      "t = 500, loss = 0.1367\n",
      "t = 600, loss = 0.4780\n",
      "t = 700, loss = 0.1244\n",
      "Got 3757 / 6690 correct (56.16)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential( \n",
    "    #########1st To Do  (10 points)###################\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 62, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(3, 128, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 128, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(3, 256, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 256, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 256, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 256, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2d(3, 512, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 512, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 512, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(3, 512, kernel_size=3, stride=1),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    \n",
    "    Flatten(), # see above for explanation\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(7, 10), # affine layer\n",
    "    nn.Linear(10, 10), # affine layer\n",
    "    nn.Linear(10, 10), # affine layer\n",
    "    ####################################\n",
    "    )\n",
    "\n",
    "fixed_model = fixed_model_base.type(dtype)\n",
    "fixed_model = fixed_model.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(fixed_model_base.parameters(), lr=1e-4)\n",
    "train(fixed_model_base, loss_fn, optimizer,image_dataloader_train, num_epochs=10) \n",
    "check_accuracy(fixed_model, image_dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I tried\n",
    "I tried the VGG-19 from ResNet paper: https://arxiv.org/pdf/1512.03385.pdf.\n",
    "The convolutional layers have a kernel_size of 3 throughout. After successive conv, activations, there are maxpool layers, with a kernel size of 2, which reduce the size of output by 2. After every Maxpooling layer, the number of filters is doubled. At the end I have used three fully connected layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9810\n"
     ]
    }
   ],
   "source": [
    "def predict_on_test(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('results.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['image']).cuda()\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i])+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "    \n",
    "count=predict_on_test(fixed_model, image_dataloader_test)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Convolution on video clips\n",
    "3D convolution is for videos, it has one more dimension than 2d convolution. Documentationu: http://pytorch.org/docs/master/nn.html#torch.nn.Conv3dIn. In our dataset, each clip is a video of 3 frames. Lets classify the each clip rather than each image using 3D convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00001\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00002\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00003\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00004\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00005\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00006\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00007\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00008\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00009\n",
      "torch.Size([3, 3, 64, 64])\n",
      "0.0\n",
      "00010\n"
     ]
    }
   ],
   "source": [
    "class ActionClipDataset(Dataset):\n",
    "    \"\"\"Action Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,  root_dir,labels=[], transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.length=len(os.listdir(self.root_dir))\n",
    "        self.labels=labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        folder=idx+1\n",
    "        folder=format(folder,'05d')\n",
    "        clip=[]\n",
    "        if len(self.labels)!=0:\n",
    "            Label=self.labels[idx][0]-1\n",
    "        for i in range(3):\n",
    "            imidx=i+1\n",
    "            imgname=str(imidx)+'.jpg'\n",
    "            img_path = os.path.join(self.root_dir,\n",
    "                                    folder,imgname)\n",
    "            image = Image.open(img_path)\n",
    "            image=np.array(image)\n",
    "            clip.append(image)\n",
    "        if self.transform:\n",
    "            clip=np.asarray(clip)\n",
    "            clip=np.transpose(clip, (0,3,1,2))\n",
    "            clip = torch.from_numpy(np.asarray(clip))\n",
    "        if len(self.labels)!=0:\n",
    "            sample={'clip':clip,'Label':Label,'folder':folder}\n",
    "        else:\n",
    "            sample={'clip':clip,'folder':folder}\n",
    "        return sample\n",
    "\n",
    "clip_dataset=ActionClipDataset(root_dir='./data/trainClips/',\\\n",
    "                               labels=label_train,transform=T.ToTensor())#/home/tqvinh/Study/CSE512/cse512-s18/hw2data/trainClips/\n",
    "for i in range(10):\n",
    "    sample=clip_dataset[i]\n",
    "    print(sample['clip'].shape)\n",
    "    print(sample['Label'])\n",
    "    print(sample['folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 3, 3, 64, 64]) ['06022', '05268', '00255', '04873'] \n",
      " 7\n",
      " 6\n",
      " 0\n",
      " 5\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "1 torch.Size([4, 3, 3, 64, 64]) ['02839', '07351', '06344', '02079'] \n",
      " 3\n",
      " 9\n",
      " 7\n",
      " 2\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "2 torch.Size([4, 3, 3, 64, 64]) ['01639', '03745', '01607', '03902'] \n",
      " 1\n",
      " 4\n",
      " 1\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "3 torch.Size([4, 3, 3, 64, 64]) ['00192', '02320', '01268', '06225'] \n",
      " 0\n",
      " 2\n",
      " 1\n",
      " 7\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "4 torch.Size([4, 3, 3, 64, 64]) ['05605', '02751', '05921', '06391'] \n",
      " 6\n",
      " 3\n",
      " 7\n",
      " 7\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "5 torch.Size([4, 3, 3, 64, 64]) ['02794', '01617', '04111', '02448'] \n",
      " 3\n",
      " 1\n",
      " 4\n",
      " 2\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "6 torch.Size([4, 3, 3, 64, 64]) ['06879', '02780', '03076', '01699'] \n",
      " 8\n",
      " 3\n",
      " 3\n",
      " 1\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "7 torch.Size([4, 3, 3, 64, 64]) ['03253', '00348', '00244', '02229'] \n",
      " 3\n",
      " 0\n",
      " 0\n",
      " 2\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "8 torch.Size([4, 3, 3, 64, 64]) ['06505', '00034', '06407', '03791'] \n",
      " 8\n",
      " 0\n",
      " 7\n",
      " 4\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "9 torch.Size([4, 3, 3, 64, 64]) ['05760', '02021', '05040', '00768'] \n",
      " 7\n",
      " 2\n",
      " 6\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "10 torch.Size([4, 3, 3, 64, 64]) ['03175', '01703', '00360', '00941'] \n",
      " 3\n",
      " 1\n",
      " 0\n",
      " 1\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "11 torch.Size([4, 3, 3, 64, 64]) ['04267', '03369', '05502', '01622'] \n",
      " 5\n",
      " 3\n",
      " 6\n",
      " 1\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "12 torch.Size([4, 3, 3, 64, 64]) ['06101', '00597', '04288', '06486'] \n",
      " 7\n",
      " 0\n",
      " 5\n",
      " 8\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "13 torch.Size([4, 3, 3, 64, 64]) ['01508', '06873', '06748', '02337'] \n",
      " 1\n",
      " 8\n",
      " 8\n",
      " 2\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "14 torch.Size([4, 3, 3, 64, 64]) ['06065', '07289', '03958', '01890'] \n",
      " 7\n",
      " 9\n",
      " 4\n",
      " 2\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "15 torch.Size([4, 3, 3, 64, 64]) ['05890', '06614', '05935', '04594'] \n",
      " 7\n",
      " 8\n",
      " 7\n",
      " 5\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "16 torch.Size([4, 3, 3, 64, 64]) ['02782', '04877', '03659', '06181'] \n",
      " 3\n",
      " 5\n",
      " 4\n",
      " 7\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "17 torch.Size([4, 3, 3, 64, 64]) ['01260', '02257', '02191', '04933'] \n",
      " 1\n",
      " 2\n",
      " 2\n",
      " 5\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "18 torch.Size([4, 3, 3, 64, 64]) ['03466', '04262', '07184', '01032'] \n",
      " 3\n",
      " 5\n",
      " 9\n",
      " 1\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "19 torch.Size([4, 3, 3, 64, 64]) ['02512', '05097', '03827', '03250'] \n",
      " 2\n",
      " 6\n",
      " 4\n",
      " 3\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "20 torch.Size([4, 3, 3, 64, 64]) ['03751', '03022', '01419', '00578'] \n",
      " 4\n",
      " 3\n",
      " 1\n",
      " 0\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n",
      "21 torch.Size([4, 3, 3, 64, 64]) ['00114', '01439', '07100', '05705'] \n",
      " 0\n",
      " 1\n",
      " 8\n",
      " 7\n",
      "[torch.DoubleTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip_dataloader = DataLoader(clip_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for i,sample in enumerate(clip_dataloader):\n",
    "    print(i,sample['clip'].shape,sample['folder'],sample['Label'])\n",
    "    if i>20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_dataset_train=ActionClipDataset(root_dir='./data/trainClips/',labels=label_train,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_train = DataLoader(clip_dataset_train, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "clip_dataset_val=ActionClipDataset(root_dir='./data/valClips/',labels=label_val,transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_val = DataLoader(clip_dataset_val, batch_size=16,\n",
    "                        shuffle=True, num_workers=4)\n",
    "clip_dataset_test=ActionClipDataset(root_dir='./data/testClips/',labels=[],transform=T.ToTensor())\n",
    "\n",
    "clip_dataloader_test = DataLoader(clip_dataset_test, batch_size=16,\n",
    "                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten function for 3d covolution feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten3d(nn.Module):\n",
    "    def forward(self, x):\n",
    "        N, C, D, H, W = x.size() # read in N, C, D, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network using 3D convolution on videos for video classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_model_3d = nn.Sequential( # You fill this in!\n",
    "    nn.Conv3d(3, 8, kernel_size=3, stride=1, padding=2),\n",
    "    nn.BatchNorm3d(8),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "    nn.Conv3d(8, 16, kernel_size=3, stride=1, padding=2),\n",
    "    nn.BatchNorm3d(16),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool3d(kernel_size=2, stride=2),\n",
    "    Flatten3d(),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(9248, 10), # affine layer\n",
    ")\n",
    "\n",
    "fixed_model_3d = fixed_model_3d.type(dtype).cuda()\n",
    "x = torch.randn(32,3, 3, 64, 64).type(dtype)\n",
    "x_var = Variable(x).type(dtype).cuda() # Construct a PyTorch Variable out of your input data\n",
    "ans = fixed_model_3d(x_var) \n",
    "np.array_equal(np.array(ans.size()), np.array([32, 10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I did\n",
    "I have used a two layer network with a fixed filter size of 3. The number of filters for 1st conv layer is 8 with padding 2, and then it is doubled (16) for the next conv layer, again with padding 2. After each convolutional layer, I have used BatchNorm. After RELU activations, I have used Max Pooling with a kernel size of 2 and stride 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.RMSprop(fixed_model_3d.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_3d(model, loss_fn, optimizer,dataloader,num_epochs = 1):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "        model.train()\n",
    "        for t, sample in enumerate(dataloader):\n",
    "            x_var = Variable(sample['clip'].type(dtype)).cuda()\n",
    "            y_var = Variable(sample['Label'].type(dtype).long()).cuda()\n",
    "\n",
    "            scores = model(x_var)\n",
    "            \n",
    "            loss = loss_fn(scores, y_var)\n",
    "            if (t + 1) % print_every == 0:\n",
    "                print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def check_accuracy_3d(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['clip'].type(dtype)).cuda()\n",
    "        y_var = sample['Label'].type(dtype).cuda()\n",
    "        scores = model(x_var)\n",
    "        y_var=y_var.cpu()\n",
    "        _, preds = scores.data.cpu().max(1)\n",
    "        #print(preds)\n",
    "        #print(y_var)\n",
    "        y_var = y_var.cpu()\n",
    "        preds = preds.cpu()\n",
    "        num_correct += (preds.numpy() == y_var.numpy()).sum()\n",
    "        num_samples += preds.size(0)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 / 10\n",
      "t = 100, loss = 1.2843\n",
      "t = 200, loss = 1.0668\n",
      "t = 300, loss = 1.0014\n",
      "t = 400, loss = 0.5186\n",
      "Starting epoch 2 / 10\n",
      "t = 100, loss = 0.6908\n",
      "t = 200, loss = 0.5488\n",
      "t = 300, loss = 0.1692\n",
      "t = 400, loss = 0.3790\n",
      "Starting epoch 3 / 10\n",
      "t = 100, loss = 0.7274\n",
      "t = 200, loss = 0.2435\n",
      "t = 300, loss = 0.1792\n",
      "t = 400, loss = 0.2836\n",
      "Starting epoch 4 / 10\n",
      "t = 100, loss = 0.3808\n",
      "t = 200, loss = 0.4830\n",
      "t = 300, loss = 0.2828\n",
      "t = 400, loss = 0.1042\n",
      "Starting epoch 5 / 10\n",
      "t = 100, loss = 0.1287\n",
      "t = 200, loss = 0.0491\n",
      "t = 300, loss = 0.0802\n",
      "t = 400, loss = 0.1436\n",
      "Starting epoch 6 / 10\n",
      "t = 100, loss = 0.0758\n",
      "t = 200, loss = 0.0392\n",
      "t = 300, loss = 0.0659\n",
      "t = 400, loss = 0.0713\n",
      "Starting epoch 7 / 10\n",
      "t = 100, loss = 0.0444\n",
      "t = 200, loss = 0.0638\n",
      "t = 300, loss = 0.0753\n",
      "t = 400, loss = 0.0290\n",
      "Starting epoch 8 / 10\n",
      "t = 100, loss = 0.0982\n",
      "t = 200, loss = 0.1066\n",
      "t = 300, loss = 0.0125\n",
      "t = 400, loss = 0.2513\n",
      "Starting epoch 9 / 10\n",
      "t = 100, loss = 0.0458\n",
      "t = 200, loss = 0.0170\n",
      "t = 300, loss = 0.1605\n",
      "t = 400, loss = 0.0498\n",
      "Starting epoch 10 / 10\n",
      "t = 100, loss = 0.1110\n",
      "t = 200, loss = 0.0440\n",
      "t = 300, loss = 0.1034\n",
      "t = 400, loss = 0.0274\n",
      "Got 1401 / 2230 correct (62.83)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.random.manual_seed(12345)\n",
    "fixed_model_3d.apply(reset) \n",
    "fixed_model_3d.train() \n",
    "train_3d(fixed_model_3d, loss_fn, optimizer,clip_dataloader_train, num_epochs=10) \n",
    "fixed_model_3d.eval() \n",
    "check_accuracy_3d(fixed_model_3d, clip_dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3270\n"
     ]
    }
   ],
   "source": [
    "def predict_on_test_3d(model, loader):\n",
    "    '''\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')  \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # Put the model in test mode (the opposite of model.train(), essentially)\n",
    "    results=open('results_3d.csv','w')\n",
    "    count=0\n",
    "    results.write('Id'+','+'Class'+'\\n')\n",
    "    for t, sample in enumerate(loader):\n",
    "        x_var = Variable(sample['clip'].type(dtype)).cuda()\n",
    "        scores = model(x_var)\n",
    "        _, preds = scores.data.max(1)\n",
    "        for i in range(len(preds)):\n",
    "            results.write(str(count)+','+str(preds[i])+'\\n')\n",
    "            count+=1\n",
    "    results.close()\n",
    "    return count\n",
    "    \n",
    "count=predict_on_test_3d(fixed_model_3d, clip_dataloader_test)\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
